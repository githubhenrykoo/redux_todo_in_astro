name: Git Log and Analysis

on:
  schedule:
    - cron: '0 0 * * *'
  workflow_dispatch:
    inputs:
      days:
        description: 'Number of days to look back'
        required: false
        default: '1'
        type: string
      query:
        description: 'What would you like to ask about the logs?'
        required: false
        default: 'Summarize the main changes'
        type: string

permissions:
  contents: write

jobs:
  generate-and-analyze:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
      with:
        fetch-depth: 0
        token: ${{ secrets.GITHUB_TOKEN }}

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.x'

    - name: Install dependencies
      run: |
        pip install --upgrade google-generativeai
        pip install python-dotenv

    - name: Generate Git Log
      run: |
        # Import name mapping
        cat << 'EOF' > get_name.py
        from Docs.config.name_mapping import NAME_MAPPING

        def get_real_name(username):
            return NAME_MAPPING.get(username, username)
        EOF

        # Generate main log file
        echo "# Git Activity Log" > "Docs/log/git-log-$(date +%Y-%m-%d).md"
        echo "Generated at: $(date)" >> "Docs/log/git-log-$(date +%Y-%m-%d).md"
        
        # Get first and last commit hashes
        FIRST_COMMIT=$(git log --since="${{ github.event.inputs.days || 1 }} days ago" --reverse --format="%H" | head -n 1)
        LAST_COMMIT=$(git log --since="${{ github.event.inputs.days || 1 }} days ago" --format="%H" | head -n 1)
        
        # Generate main diff log
        echo "## Changes Between First and Last Commits" >> "Docs/log/git-log-$(date +%Y-%m-%d).md"
        echo "\`\`\`diff" >> "Docs/log/git-log-$(date +%Y-%m-%d).md"
        if [ ! -z "$FIRST_COMMIT" ] && [ ! -z "$LAST_COMMIT" ]; then
          git diff $FIRST_COMMIT..$LAST_COMMIT -- . ':!node_modules' ':!package-lock.json' >> "Docs/log/git-log-$(date +%Y-%m-%d).md"
        else
          echo "No commits found in the specified timeframe" >> "Docs/log/git-log-$(date +%Y-%m-%d).md"
        fi
        echo "\`\`\`" >> "Docs/log/git-log-$(date +%Y-%m-%d).md"
        
        # Generate per-user logs with real names
        for author in $(git log --since="${{ github.event.inputs.days || 1 }} days ago" --format="%ae" | sort -u); do
          username=$(echo "$author" | cut -d@ -f1)
          real_name=$(python3 -c "from get_name import get_real_name; print(get_real_name('$username'))")
          mkdir -p "Docs/log/users/$username"
          
          echo "# Git Activity Log - $real_name" > "Docs/log/users/$username/git-log-$(date +%Y-%m-%d).md"
          echo "Generated at: $(date)" >> "Docs/log/users/$username/git-log-$(date +%Y-%m-%d).md"
          echo "## Changes by $real_name" >> "Docs/log/users/$username/git-log-$(date +%Y-%m-%d).md"
          echo "\`\`\`diff" >> "Docs/log/users/$username/git-log-$(date +%Y-%m-%d).md"
          git log --since="${{ github.event.inputs.days || 1 }} days ago" --author="$author" --patch --no-merges -- . ':!node_modules' ':!package-lock.json' >> "Docs/log/users/$username/git-log-$(date +%Y-%m-%d).md"
          echo "\`\`\`" >> "Docs/log/users/$username/git-log-$(date +%Y-%m-%d).md"
        done

    - name: Analyze Logs with Gemini
      env:
        GOOGLE_API_KEY: AIzaSyBZ52gRnYBjfyyh4jiEWscKoRfTx-j4YEQ
      run: |
        cat << 'EOF' > analyze_logs.py
        import os
        import glob
        import time
        from datetime import datetime
        import google.generativeai as genai
        from google.api_core import exceptions
        from Docs.config.prompts.group_analysis import GROUP_ANALYSIS_PROMPT
        from Docs.config.prompts.user_analysis import USER_ANALYSIS_PROMPT
        from Docs.config.prompts.summary import SUMMARY_PROMPT

        def generate_with_retry(model, prompt, max_retries=3, initial_delay=5):
            for attempt in range(max_retries):
                try:
                    if attempt > 0:
                        time.sleep(initial_delay * (2 ** attempt))  # Exponential backoff
                    response = model.generate_content(prompt)
                    return response.text
                except exceptions.ResourceExhausted:
                    if attempt == max_retries - 1:
                        raise
                    print(f"Rate limit hit, retrying in {initial_delay * (2 ** (attempt + 1))} seconds...")
                except Exception as e:
                    print(f"Error: {str(e)}")
                    if attempt == max_retries - 1:
                        raise
            return None

        def analyze_content(model, content, query, prompt_template):
            chunks = chunk_content(content)
            all_analyses = []
            
            for i, chunk in enumerate(chunks, 1):
                if i > 1:
                    time.sleep(5)  # Increased delay between requests
                
                chunk_prompt = prompt_template.format(
                    query=query,
                    content=chunk,
                    chunk_info=f"(Part {i} of {len(chunks)})" if len(chunks) > 1 else ""
                )
                
                analysis = generate_with_retry(model, chunk_prompt)
                if analysis:
                    all_analyses.append(analysis)
            
            if len(all_analyses) > 1:
                time.sleep(5)  # Increased delay before summary
                summary_prompt = SUMMARY_PROMPT.format(content='\n\n'.join(all_analyses))
                return generate_with_retry(model, summary_prompt)
            
            return all_analyses[0] if all_analyses else "Analysis failed due to API limitations"

        def chunk_content(content, max_chars=400000):  # Approximately 100k tokens
            lines = content.split('\n')
            chunks = []
            current_chunk = []
            current_size = 0
            
            for line in lines:
                line_size = len(line) + 1  # +1 for newline
                if current_size + line_size > max_chars and current_chunk:
                    chunks.append('\n'.join(current_chunk))
                    current_chunk = [line]
                    current_size = line_size
                else:
                    current_chunk.append(line)
                    current_size += line_size
            
            if current_chunk:
                chunks.append('\n'.join(current_chunk))
            return chunks

        # Configure Gemini
        genai.configure(api_key="AIzaSyBZ52gRnYBjfyyh4jiEWscKoRfTx-j4YEQ")
        model = genai.GenerativeModel('gemini-2.0-flash')

        # Analyze group log
        log_files = glob.glob('Docs/log/git-log-*.md')
        if log_files:
            latest_log = max(log_files)
            with open(latest_log, 'r') as f:
                group_content = f.read()

            query = '${{ github.event.inputs.query }}'
            analysis = analyze_content(model, group_content, query, GROUP_ANALYSIS_PROMPT)
            os.makedirs('Docs/analysis/group', exist_ok=True)
            with open(f'Docs/analysis/group/team-analysis-{datetime.now().strftime("%Y-%m-%d")}.md', 'w') as f:
                f.write(f"# Team Analysis\nGenerated at: {datetime.now()}\n\n{analysis}")

        # Analyze individual user logs
        user_dirs = glob.glob('Docs/log/users/*/')
        for user_dir in user_dirs:
            username = os.path.basename(os.path.dirname(user_dir))
            if username == '.gitkeep':
                continue

            user_logs = glob.glob(f'{user_dir}git-log-*.md')
            if user_logs:
                latest_user_log = max(user_logs)
                with open(latest_user_log, 'r') as f:
                    user_content = f.read()

                response = model.generate_content(USER_ANALYSIS_PROMPT.format(
                    query=query,
                    content=user_content
                ))
                os.makedirs(f'Docs/analysis/users/{username}', exist_ok=True)
                with open(f'Docs/analysis/users/{username}/analysis-{datetime.now().strftime("%Y-%m-%d")}.md', 'w') as f:
                    f.write(f"# Developer Analysis - {username}\nGenerated at: {datetime.now()}\n\n{response.text}")
        EOF

        python analyze_logs.py

    - name: Refine Analysis
      env:
        GOOGLE_API_KEY: AIzaSyBZ52gRnYBjfyyh4jiEWscKoRfTx-j4YEQ
      run: |
        cat << 'EOF' > refine_analysis.py
        import os
        import glob
        from datetime import datetime
        import google.generativeai as genai
        from Docs.config.prompts.group_critique import GROUP_CRITIQUE_PROMPT
        from Docs.config.prompts.user_critique import USER_CRITIQUE_PROMPT
        from Docs.config.prompts.refinement import REFINEMENT_PROMPT

        # Replace hardcoded prompts with imported ones
        EOF

        python refine_analysis.py

    - name: Commit and Push Changes
      run: |
        git config --local user.email "github-actions[bot]@users.noreply.github.com"
        git config --local user.name "github-actions[bot]"
        git add "Docs/log/" "Docs/analysis/" "analyze_logs.py"
        git commit -m "docs: update git log and analysis for $(date +%Y-%m-%d)" || echo "No changes to commit"
        # Pull changes first with rebase strategy
        git pull --rebase origin main
        # Now push the changes
        git push origin HEAD:main