name: Git Log and Analysis (Alternative)

on:
  schedule:
    - cron: '0 0 * * *'
  workflow_dispatch:
    inputs:
      days:
        description: 'Number of days to look back'
        required: false
        default: '1'
        type: string
      query:
        description: 'What would you like to ask about the logs?'
        required: false
        default: 'Summarize the main changes'
        type: string

permissions:
  contents: write

env:
  GOOGLE_API_KEY: AIzaSyBZ52gRnYBjfyyh4jiEWscKoRfTx-j4YEQ

jobs:
  generate-and-analyze:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
      with:
        fetch-depth: 0
        token: ${{ secrets.GITHUB_TOKEN }}

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.x'

    - name: Install dependencies
      run: |
        pip install --upgrade google-generativeai
        pip install python-dotenv

    - name: Generate Git Log
      run: |
        # Import name mapping
        cat << 'EOF' > get_name.py
        from Docs.config.name_mapping import NAME_MAPPING

        def get_real_name(username):
            return NAME_MAPPING.get(username, username)
        EOF

        # Generate main log file
        echo "# Git Activity Log" > "Docs/log/git-log-$(date +%Y-%m-%d).md"
        echo "Generated at: $(date)" >> "Docs/log/git-log-$(date +%Y-%m-%d).md"
        
        # Get first and last commit hashes
        FIRST_COMMIT=$(git log --since="${{ github.event.inputs.days || 1 }} days ago" --reverse --format="%H" | head -n 1)
        LAST_COMMIT=$(git log --since="${{ github.event.inputs.days || 1 }} days ago" --format="%H" | head -n 1)
        
        # Generate main diff log
        echo "## Changes Between First and Last Commits" >> "Docs/log/git-log-$(date +%Y-%m-%d).md"
        echo "\`\`\`diff" >> "Docs/log/git-log-$(date +%Y-%m-%d).md"
        if [ ! -z "$FIRST_COMMIT" ] && [ ! -z "$LAST_COMMIT" ]; then
          git diff $FIRST_COMMIT..$LAST_COMMIT -- . ':!node_modules' ':!package-lock.json' >> "Docs/log/git-log-$(date +%Y-%m-%d).md"
        else
          echo "No commits found in the specified timeframe" >> "Docs/log/git-log-$(date +%Y-%m-%d).md"
        fi
        echo "\`\`\`" >> "Docs/log/git-log-$(date +%Y-%m-%d).md"
        
        # Generate per-user logs with real names
        for author in $(git log --since="${{ github.event.inputs.days || 1 }} days ago" --format="%ae" | sort -u); do
          username=$(echo "$author" | cut -d@ -f1)
          real_name=$(python3 -c "from get_name import get_real_name; print(get_real_name('$username'))")
          mkdir -p "Docs/log/users/$username"
          
          echo "# Git Activity Log - $real_name" > "Docs/log/users/$username/git-log-$(date +%Y-%m-%d).md"
          echo "Generated at: $(date)" >> "Docs/log/users/$username/git-log-$(date +%Y-%m-%d).md"
          echo "## Changes by $real_name" >> "Docs/log/users/$username/git-log-$(date +%Y-%m-%d).md"
          echo "\`\`\`diff" >> "Docs/log/users/$username/git-log-$(date +%Y-%m-%d).md"
          git log --since="${{ github.event.inputs.days || 1 }} days ago" --author="$author" --patch --no-merges -- . ':!node_modules' ':!package-lock.json' >> "Docs/log/users/$username/git-log-$(date +%Y-%m-%d).md"
          echo "\`\`\`" >> "Docs/log/users/$username/git-log-$(date +%Y-%m-%d).md"
        done

    - name: Analyze Logs with Gemini
      env:
        GOOGLE_API_KEY: AIzaSyBZ52gRnYBjfyyh4jiEWscKoRfTx-j4YEQ
      run: |
        cat << 'EOF' > analyze_logs.py
        import os
        import glob
        import time
        from datetime import datetime
        import google.generativeai as genai
        from Docs.config.prompts.group_analysis import GROUP_ANALYSIS_PROMPT
        from Docs.config.prompts.user_analysis import USER_ANALYSIS_PROMPT
        from Docs.config.prompts.summary import SUMMARY_PROMPT

        def generate_with_retry(model, prompt, max_retries=3, initial_delay=5):
            for attempt in range(max_retries):
                try:
                    if attempt > 0:
                        time.sleep(initial_delay * (2 ** attempt))  # Exponential backoff
                    response = model.generate_content(prompt)
                    return response.text
                except exceptions.ResourceExhausted:
                    if attempt == max_retries - 1:
                        raise
                    print(f"Rate limit hit, retrying in {initial_delay * (2 ** (attempt + 1))} seconds...")
                except Exception as e:
                    print(f"Error: {str(e)}")
                    if attempt == max_retries - 1:
                        raise
            return None

        def analyze_content(model, content, query, prompt_template):
            chunks = chunk_content(content)
            all_analyses = []
            
            for i, chunk in enumerate(chunks, 1):
                if i > 1:
                    time.sleep(5)  # Increased delay between requests
                
                chunk_prompt = prompt_template.format(
                    query=query,
                    content=chunk,
                    chunk_info=f"(Part {i} of {len(chunks)})" if len(chunks) > 1 else ""
                )
                
                analysis = generate_with_retry(model, chunk_prompt)
                if analysis:
                    all_analyses.append(analysis)
            
            if len(all_analyses) > 1:
                time.sleep(5)  # Increased delay before summary
                summary_prompt = SUMMARY_PROMPT.format(content='\n\n'.join(all_analyses))
                return generate_with_retry(model, summary_prompt)
            
            return all_analyses[0] if all_analyses else "Analysis failed due to API limitations"

        def chunk_content(content, max_chars=400000):  # Approximately 100k tokens
            lines = content.split('\n')
            chunks = []
            current_chunk = []
            current_size = 0
            
            for line in lines:
                line_size = len(line) + 1  # +1 for newline
                if current_size + line_size > max_chars and current_chunk:
                    chunks.append('\n'.join(current_chunk))
                    current_chunk = [line]
                    current_size = line_size
                else:
                    current_chunk.append(line)
                    current_size += line_size
            
            if current_chunk:
                chunks.append('\n'.join(current_chunk))
            return chunks

        # Configure Gemini
        genai.configure(api_key="AIzaSyBZ52gRnYBjfyyh4jiEWscKoRfTx-j4YEQ")
        model = genai.GenerativeModel('gemini-2.0-flash')

        # Analyze group log
        log_files = glob.glob('Docs/log/git-log-*.md')
        if log_files:
            latest_log = max(log_files)
            with open(latest_log, 'r') as f:
                group_content = f.read()

            query = '${{ github.event.inputs.query }}'
            analysis = analyze_content(model, group_content, query, GROUP_ANALYSIS_PROMPT)
            os.makedirs('Docs/analysis/group', exist_ok=True)
            with open(f'Docs/analysis/group/team-analysis-{datetime.now().strftime("%Y-%m-%d")}.md', 'w') as f:
                f.write(f"# Team Analysis\nGenerated at: {datetime.now()}\n\n{analysis}")

        # Analyze individual user logs
        user_dirs = glob.glob('Docs/log/users/*/')
        for user_dir in user_dirs:
            username = os.path.basename(os.path.dirname(user_dir))
            if username == '.gitkeep':
                continue

            user_logs = glob.glob(f'{user_dir}git-log-*.md')
            if user_logs:
                latest_user_log = max(user_logs)
                with open(latest_user_log, 'r') as f:
                    user_content = f.read()

                response = model.generate_content(USER_ANALYSIS_PROMPT.format(
                    query=query,
                    content=user_content
                ))
                os.makedirs(f'Docs/analysis/users/{username}', exist_ok=True)
                with open(f'Docs/analysis/users/{username}/analysis-{datetime.now().strftime("%Y-%m-%d")}.md', 'w') as f:
                    f.write(f"# Developer Analysis - {username}\nGenerated at: {datetime.now()}\n\n{response.text}")
        EOF

        python analyze_logs.py

    - name: Refine Analysis
      env:
        GOOGLE_API_KEY: AIzaSyBZ52gRnYBjfyyh4jiEWscKoRfTx-j4YEQ
      run: |
        cat << 'EOF' > refine_analysis.py
        import os
        import glob
        import time
        from datetime import datetime
        import google.generativeai as genai
        from google.api_core import exceptions
        from Docs.config.prompts.group_critique import GROUP_CRITIQUE_PROMPT
        from Docs.config.prompts.user_critique import USER_CRITIQUE_PROMPT
        from Docs.config.prompts.refinement import REFINEMENT_PROMPT

        def generate_with_retry(model, prompt, max_retries=3, initial_delay=5):
            for attempt in range(max_retries):
                try:
                    if attempt > 0:
                        time.sleep(initial_delay * (2 ** attempt))
                    response = model.generate_content(prompt)
                    return response.text
                except exceptions.ResourceExhausted:
                    if attempt == max_retries - 1:
                        raise
                    print(f"Rate limit hit, retrying in {initial_delay * (2 ** (attempt + 1))} seconds...")
                except Exception as e:
                    print(f"Error: {str(e)}")
                    if attempt == max_retries - 1:
                        raise
            return None

        def refine_analysis(model, analysis_content, critique_prompt):
            # Generate critique
            critique = generate_with_retry(model, critique_prompt)
            if not critique:
                return analysis_content

            # Use critique to refine
            refined = generate_with_retry(
                model,
                REFINEMENT_PROMPT.format(
                    analysis_content=analysis_content,
                    critique=critique
                )
            )
            return refined if refined else analysis_content

        # Configure Gemini
        genai.configure(api_key=os.getenv('GOOGLE_API_KEY'))
        model = genai.GenerativeModel('gemini-2.0-flash')

        # Refine group analysis
        group_files = glob.glob('Docs/analysis/group/team-analysis-*.md')
        if group_files:
            latest_analysis = max(group_files)
            with open(latest_analysis, 'r') as f:
                analysis_content = f.read()
            
            refined_analysis = refine_analysis(model, analysis_content, GROUP_CRITIQUE_PROMPT)
            if refined_analysis:
                refined_path = latest_analysis.replace('team-analysis-', 'refined-team-analysis-')
                with open(refined_path, 'w') as f:
                    f.write(f"# Refined Team Analysis\nGenerated at: {datetime.now()}\n\n{refined_analysis}")

        # Refine individual analyses
        user_dirs = glob.glob('Docs/analysis/users/*/')
        for user_dir in user_dirs:
            username = os.path.basename(os.path.dirname(user_dir))
            if username == '.gitkeep':
                continue

            analysis_files = glob.glob(f'{user_dir}analysis-*.md')
            if analysis_files:
                latest_analysis = max(analysis_files)
                with open(latest_analysis, 'r') as f:
                    analysis_content = f.read()

                refined_analysis = refine_analysis(model, analysis_content, USER_CRITIQUE_PROMPT)
                if refined_analysis:
                    refined_path = latest_analysis.replace('analysis-', 'refined-analysis-')
                    with open(refined_path, 'w') as f:
                        f.write(f"# Refined Developer Analysis - {username}\nGenerated at: {datetime.now()}\n\n{refined_analysis}")
        EOF

        python refine_analysis.py

    - name: Commit and Push Changes
      run: |
        git config --local user.email "github-actions[bot]@users.noreply.github.com"
        git config --local user.name "github-actions[bot]"
        
        # Clean up Python cache files
        find . -type d -name "__pycache__" -exec rm -r {} +
        
        # Stage specific files and directories
        git add \
          "Docs/log/" \
          "Docs/analysis/" \
          "analyze_logs.py" \
          "get_name.py" \
          "refine_analysis.py"
        
        # Check if there are changes to commit
        if git diff --staged --quiet; then
          echo "No changes to commit"
          exit 0
        fi
        
        # Pull latest changes
        git pull origin main --no-rebase
        
        # Commit changes
        git commit -m "docs: update git log and analysis for $(date +%Y-%m-%d)"
        
        # Push changes
        git push origin main

  refine-meta-template:
    needs: generate-and-analyze
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
      with:
        fetch-depth: 0
        token: ${{ secrets.GITHUB_TOKEN }}

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.x'

    - name: Install dependencies
      run: |
        pip install --upgrade google-generativeai
        pip install python-dotenv

    - name: Create Documentation from Template
      env:
        GOOGLE_API_KEY: AIzaSyBZ52gRnYBjfyyh4jiEWscKoRfTx-j4YEQ
      run: |
        cat << 'EOF' > create_docs.py
        import os
        import glob
        import time
        from datetime import datetime
        import google.generativeai as genai
        from google.api_core import exceptions

        def generate_with_retry(model, prompt, max_retries=3, initial_delay=60):
            for attempt in range(max_retries):
                try:
                    if attempt > 0:
                        # Exponential backoff with longer initial delay
                        delay = initial_delay * (2 ** attempt)
                        print(f"Waiting {delay} seconds before retry {attempt + 1}...")
                        time.sleep(delay)
                    response = model.generate_content(prompt)
                    # Add delay after successful request to respect rate limits
                    time.sleep(10)
                    return response.text
                except exceptions.ResourceExhausted as e:
                    print(f"Rate limit hit on attempt {attempt + 1}: {str(e)}")
                    if attempt == max_retries - 1:
                        raise
                except Exception as e:
                    print(f"Error on attempt {attempt + 1}: {str(e)}")
                    if attempt == max_retries - 1:
                        raise
            return None

        def fill_template(model, content, username=None):
            # Read template
            with open('Docs/analysis/template/meta_template.md', 'r') as f:
                template = f.read()

            # Generate content for each section
            replacements = {
                '[Document Type]': 'Git Analysis Report',
                '[Title]': f'Development Analysis - {username if username else "Team"}',
                '[Team Members]': 'AI Analysis System',
                '[YYYY-MM-DD]': datetime.now().strftime('%Y-%m-%d'),
                '[X.Y]': '1.0',
                '[Link to GitHub Repository if needed]': os.getenv('GITHUB_REPOSITORY', 'Current Repository'),
                '[Planning/Report/Review/Implementation]': 'Analysis Report',
            }

            # Generate Executive Summary
            exec_summary = generate_with_retry(model, 
                f"Generate an executive summary for git analysis using this format:\n"
                f"Logic: Core purpose and objectives\n"
                f"Implementation: Key processes and methods\n"
                f"Outcomes: Results\n\nAnalyze:\n{content}")
            replacements['[One-paragraph overview using Computational Trinitarianism framework:\n- Logic: Core purpose and formal objectives\n- Implementation: Key processes and methods\n- Outcomes: Expected or achieved results]'] = exec_summary

            # Generate Context & Vision
            context = generate_with_retry(model, f"Analyze git activity context:\n{content}")
            replacements['[Boundaries and limitations]'] = context
            replacements['[Environmental factors]'] = context
            replacements['[Involved parties]'] = context

            # Fill in other sections
            sections = {
                '[Data/Resources]': 'Git Repository Data',
                '[Transformation]': 'Analysis and Processing',
                '[Expected results]': 'Development Insights',
                '[Quality checks]': 'Automated Analysis',
                '[Learning loops]': 'Continuous Improvement',
                '[Measurable outcomes]': generate_with_retry(model, f"List quantitative metrics from:\n{content}"),
                '[Observable improvements]': generate_with_retry(model, f"List qualitative improvements from:\n{content}"),
                '[Verification approaches]': 'Automated and Manual Verification',
                '[Regional factors]': 'Development Team Context',
                '[Communication needs]': 'Technical Documentation',
                '[Social dynamics]': 'Team Collaboration Patterns',
                '[AI assistance points]': 'Gemini AI Analysis',
                '[Sensor/Actuator needs]': 'Git Event Monitoring',
                '[Connectivity specs]': 'GitHub API Integration'
            }
            replacements.update(sections)

            # Development Workflow sections
            workflow_sections = generate_with_retry(model, 
                f"Analyze the development workflow stages from git history:\n{content}")
            replacements['[Functions deployed]'] = workflow_sections
            replacements['[Success metrics]'] = workflow_sections
            replacements['[Technical setup]'] = workflow_sections
            replacements['[Capability building]'] = workflow_sections

            # Evidence and Outcomes
            evidence = generate_with_retry(model, 
                f"Extract evidence and outcomes from git history:\n{content}")
            replacements['[Key indicators]'] = evidence
            replacements['[Standards]'] = evidence
            replacements['[Results]'] = evidence

            # Replace all placeholders in template
            doc_content = template
            for key, value in replacements.items():
                if value:
                    doc_content = doc_content.replace(key, value)

            return doc_content

        # Configure Gemini
        genai.configure(api_key=os.getenv('GOOGLE_API_KEY'))
        model = genai.GenerativeModel('gemini-2.0-flash')

        # Process team analysis
        team_files = glob.glob('Docs/analysis/group/team-analysis-*.md')
        if team_files:
            latest_team = max(team_files)
            with open(latest_team, 'r') as f:
                team_content = f.read()
            
            formatted_content = fill_template(model, team_content)
            output_path = latest_team.replace('team-analysis-', 'formatted-team-analysis-')
            with open(output_path, 'w') as f:
                f.write(formatted_content)

        # Process individual analyses
        user_dirs = glob.glob('Docs/analysis/users/*/')
        for user_dir in user_dirs:
            username = os.path.basename(os.path.dirname(user_dir))
            if username == '.gitkeep':
                continue

            analysis_files = glob.glob(f'{user_dir}analysis-*.md')
            if analysis_files:
                latest = max(analysis_files)
                with open(latest, 'r') as f:
                    content = f.read()
                
                formatted_content = fill_template(model, content, username)
                output_path = latest.replace('analysis-', 'formatted-analysis-')
                with open(output_path, 'w') as f:
                    f.write(formatted_content)
        EOF

        python create_docs.py

    - name: Commit and Push Changes
      run: |
        git config --local user.email "github-actions[bot]@users.noreply.github.com"
        git config --local user.name "github-actions[bot]"
        git add "Docs/analysis/"
        git commit -m "docs: create formatted analysis documents $(date +%Y-%m-%d)" || echo "No changes to commit"
        git pull --rebase origin main
        git push origin HEAD:main
