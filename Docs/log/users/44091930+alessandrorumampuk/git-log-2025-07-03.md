# Git Activity Log - 44091930+alessandrorumampuk
Generated at: Thu Jul  3 00:48:23 UTC 2025
## Changes by 44091930+alessandrorumampuk
```diff
commit ae3615eaf32d5d228210cb24520c1b93eb49f621
Author: Alessandro Rumampuk <44091930+alessandrorumampuk@users.noreply.github.com>
Date:   Wed Jul 2 15:06:33 2025 +0800

    Update docker-compose.yml

diff --git a/docker-compose.yml b/docker-compose.yml
index 75671e6..0bccf49 100644
--- a/docker-compose.yml
+++ b/docker-compose.yml
@@ -1,17 +1,18 @@
-version: "3.8"
 services:
   app:
-    build:
-      context: .
-      args:
-        BUILD_MODE: nocheck
+    image: henry768/redux_todo_in_astro:last
+    container_name: redux-todo-astro
     ports:
       - "4321:4321"
-    volumes:
-      - .:/app
-      - /app/node_modules
-    environment:
-      - NODE_ENV=development
-      - HOST=0.0.0.0
-      - PORT=4321
+    env_file:
+      - .env
     command: npm run dev
+
+
+  ollama:
+    image: ollama/ollama:latest
+    container_name: ollama
+    ports:
+      - "11435:11434"
+    entrypoint: ["/bin/sh", "-c"]
+    command: ["ollama serve & sleep 5 && ollama pull qwen3:0.6b && wait"]

commit a0891d2d475cc3ed0ea26f1540ef4a5762124a0d
Author: Alessandro Rumampuk <44091930+alessandrorumampuk@users.noreply.github.com>
Date:   Wed Jul 2 15:06:06 2025 +0800

    Update chatbot.jsx

diff --git a/src/components/panels/chatbot.jsx b/src/components/panels/chatbot.jsx
index d0c619d..d12e79c 100644
--- a/src/components/panels/chatbot.jsx
+++ b/src/components/panels/chatbot.jsx
@@ -27,6 +27,12 @@ const ChatbotPanel = ({ className = '' }) => {
   const [error, setError] = useState(null);
   const [models, setModels] = useState([]);
   const [selectedModel, setSelectedModel] = useState('llama3:8b');
+  const [selectedPort, setSelectedPort] = useState('11434');
+  const [ollamaInstance, setOllamaInstance] = useState('local');
+  const [instanceStatus, setInstanceStatus] = useState({
+    local: { connected: false, models: [] },
+    docker: { connected: false, models: [] }
+  }); // 'local' or 'docker'
   
   const messagesEndRef = useRef(null);
   const inputRef = useRef(null);
@@ -41,7 +47,7 @@ const ChatbotPanel = ({ className = '' }) => {
   // Check if Ollama is running and get available models
   useEffect(() => {
     checkOllamaStatus();
-  }, []);
+  }, [selectedPort]);
   
   // Function to fetch data from the API endpoint for RAG
   const fetchExternalData = async () => {
@@ -162,12 +168,22 @@ const ChatbotPanel = ({ className = '' }) => {
   };
 
   const checkOllamaStatus = async () => {
+    const instance = selectedPort === '11434' ? 'local' : 'docker';
+    setInstanceStatus(prev => ({
+      ...prev,
+      [instance]: { ...prev[instance], connected: false }
+    }));
     try {
-      const response = await fetch('http://127.0.0.1:11434/api/tags');
+      const response = await fetch(`http://127.0.0.1:${selectedPort}/api/tags`);
       if (response.ok) {
         const data = await response.json();
         if (data.models && data.models.length > 0) {
+          const instance = selectedPort === '11434' ? 'local' : 'docker';
           setModels(data.models);
+          setInstanceStatus(prev => ({
+            ...prev,
+            [instance]: { connected: true, models: data.models }
+          }));
           // If llama3:8b is available, use it
           const llama3Model = data.models.find(model => model.name === 'llama3:8b');
           if (llama3Model) {
@@ -180,7 +196,8 @@ const ChatbotPanel = ({ className = '' }) => {
       }
     } catch (err) {
       console.error('Error checking Ollama status:', err);
-      setError('Cannot connect to Ollama server. Make sure Ollama is running at http://127.0.0.1:11434');
+      const instanceType = selectedPort === '11434' ? 'local' : 'Docker';
+      setError(`Cannot connect to ${instanceType} Ollama server. Make sure ${instanceType} Ollama is running at http://127.0.0.1:${selectedPort}`);
     }
   };
 
@@ -420,7 +437,7 @@ Answer based on the above context.`
       console.log('Sending request to Ollama API with', messagesForModel.length, 'messages');
       console.log('First message role:', messagesForModel[0]?.role);
       
-      const response = await fetch(`http://127.0.0.1:11434/api/chat`, {
+      const response = await fetch(`http://127.0.0.1:${selectedPort}/api/chat`, {
         method: 'POST',
         headers: {
           'Content-Type': 'application/json',
@@ -514,7 +531,23 @@ Answer based on the above context.`
     <div className={`h-full w-full flex flex-col bg-gray-900 text-gray-200 overflow-hidden ${className}`}>
       <div className="p-2 bg-gray-800 border-b border-gray-700 flex flex-col">
         <div className="flex items-center mb-2">
-          <div className="text-center flex-grow"><b>Chatbot</b></div>
+          <div className="flex items-center space-x-2 mr-4">
+            <div className={`w-2 h-2 rounded-full ${instanceStatus.local.connected ? 'bg-green-500' : 'bg-red-500'}`} />
+            <div className={`w-2 h-2 rounded-full ${instanceStatus.docker.connected ? 'bg-green-500' : 'bg-red-500'}`} />
+          </div>
+          <div className="text-center flex-grow"><b>Chatbot ({ollamaInstance === 'local' ? 'Local' : 'Docker'} Instance)</b></div>
+          <select 
+            value={selectedPort}
+            onChange={(e) => {
+              const port = e.target.value;
+              setSelectedPort(port);
+              setOllamaInstance(port === '11434' ? 'local' : 'docker');
+            }}
+            className="mr-2 px-2 py-1 text-xs bg-gray-700 text-white rounded"
+          >
+            <option value="11434">Local Ollama (11434)</option>
+            <option value="11435">Docker Ollama (11435)</option>
+          </select>
           <select 
             value={selectedModel}
             onChange={handleModelChange}
```
