# Git Activity Log - Daffa Padantya
Generated at: Thu Mar  6 06:28:03 UTC 2025
## Changes by Daffa Padantya
```diff
commit 1a399f89bfaccc52afda26d19d57e324c90d294e
Author: daffa <daffa.padantya12@gmail.com>
Date:   Thu Mar 6 14:27:14 2025 +0800

    prompt push

diff --git a/Docs/config/prompts/meta_template.py b/Docs/config/prompts/meta_template.py
index 7d65185..761f753 100644
--- a/Docs/config/prompts/meta_template.py
+++ b/Docs/config/prompts/meta_template.py
@@ -1,49 +1,102 @@
+# Base template structure
+BASE_TEMPLATE = """
+# {title}
 
-# Git Analysis Report
-
-**Type:** Analysis Document
+**Type:** {document_type}
 
 **1. Document Header**
-
+{header_content}
 
 **Executive Summary**
-Okay, I'm ready! To give you a concise executive summary, I need to know what it's about. Please tell me:
-
-*   **What is the subject of the summary?** (e.g., a project, a report, a business plan, a research paper, a meeting)
-*   **What are the main objectives or goals?** (What was the purpose of the subject you're summarizing?)
-*   **What are the key findings or conclusions?** (What are the most important takeaways?)
-*   **What are the recommendations or next steps?** (What should be done based on the findings?)
-*   **Who is the intended audience?** (Executives need different information than technical staff, for example.)
-
-Once you give me this information, I can craft a concise and effective executive summary.
-
-
-**2. Analysis Framework**
+{executive_summary}
+"""
+
+# Section templates
+HEADER_TEMPLATE = """
+**1.1 Title and Type**
+* **Title:** {title}
+* **Type:** {document_type}
+
+**1.2 Metadata**
+* **Authors:** {authors}
+* **Date:** {date}
+* **Version:** {version}
+* **Repository:** {repository}
+* **Hash:** {hash}
+* **Category:** {category}
+"""
+
+FRAMEWORK_TEMPLATE = """
+**2. {framework_name} Framework**
 
 **2.a. Logic Layer**
-
+{logic_content}
 
 **2.b. Implementation Layer**
-
+{implementation_content}
 
 **2.c. Evidence Layer**
+{evidence_content}
+"""
 
-
-
+MANAGEMENT_TEMPLATE = """
 **3. Management Framework**
 * **Budget Structure:**
-
+{budget_content}
 
 * **Timeline Management:**
-
+{timeline_content}
 
 * **Integration Matrix:**
+{integration_content}
+"""
 
-
-
+DOCUMENTATION_TEMPLATE = """
 **4. Supporting Documentation**
 * **References:**
-
+{references}
 
 * **Change History:**
-
+{change_history}
+"""
+
+# Validation criteria for each section
+VALIDATION_CRITERIA = {
+    'header': ['title', 'type', 'metadata'],
+    'executive_summary': ['context', 'goals', 'approach', 'expected_outcomes'],
+    'framework': ['logic', 'implementation', 'evidence'],
+    'management': ['budget', 'timeline', 'integration'],
+    'documentation': ['references', 'changes']
+}
+
+# Section-specific prompts
+SECTION_PROMPTS = {
+    'header': 'Generate a document header with title, type, and metadata...',
+    'executive_summary': 'Create a concise executive summary that covers...',
+    'framework': 'Develop a framework section that includes...',
+    'management': 'Structure the management section with...',
+    'documentation': 'Compile supporting documentation including...'
+}
+
+# Template assembly function
+def assemble_template(sections):
+    return '\n'.join([
+        BASE_TEMPLATE,
+        FRAMEWORK_TEMPLATE,
+        MANAGEMENT_TEMPLATE,
+        DOCUMENTATION_TEMPLATE
+    ]).format(**sections)
+
+# Meta template prompt
+META_TEMPLATE_PROMPT = """
+Analyze the git repository activity and generate a detailed report that includes:
+
+1. Team Overview: Analyze collaboration patterns and team dynamics
+2. Code Changes: Review significant code modifications and their impact
+3. Development Trends: Identify patterns in development activity
+4. Performance Metrics: Measure commit frequency, code quality, and review cycles
+5. Recommendations: Suggest improvements based on the analysis
+
+Git repository content to analyze:
+{content}
+"""
\ No newline at end of file

commit d69ca3a1b1aca9a6aa9245728e6bd6774c751a04
Author: daffa <daffa.padantya12@gmail.com>
Date:   Thu Mar 6 13:30:21 2025 +0800

    update refinement template

diff --git a/.github/workflows/git_analysis.yml b/.github/workflows/git_analysis.yml
index 0b34813..210c8a8 100644
--- a/.github/workflows/git_analysis.yml
+++ b/.github/workflows/git_analysis.yml
@@ -371,11 +371,35 @@ jobs:
             return generate_with_retry(model, prompt)
 
         def refine_template(model, template_content):
-            sections = {}
+            # Default values for required fields
+            sections = {
+                'title': 'Git Analysis Report',
+                'document_type': 'Analysis Document',
+                'authors': 'AI Analysis System',
+                'date': datetime.now().strftime('%Y-%m-%d'),
+                'version': '1.0',
+                'repository': 'Current Repository',
+                'hash': 'Generated',
+                'category': 'Git Analysis',
+                'header_content': '',
+                'executive_summary': '',
+                'framework_name': 'Analysis',
+                'logic_content': '',
+                'implementation_content': '',
+                'evidence_content': '',
+                'budget_content': '',
+                'timeline_content': '',
+                'integration_content': '',
+                'references': '',
+                'change_history': ''
+            }
+            
             # Refine each section separately
             for section in VALIDATION_CRITERIA.keys():
-                sections[section] = refine_section(model, section, template_content)
-                time.sleep(2)  # Rate limiting
+                refined_content = refine_section(model, section, template_content)
+                if refined_content:
+                    sections[section] = refined_content
+                time.sleep(2)
             
             # Assemble final template
             return assemble_template(sections)

commit fda7fa22faef58e17efdd0787e9c2311ca0980f4
Author: daffa <daffa.padantya12@gmail.com>
Date:   Thu Mar 6 13:12:01 2025 +0800

    prompt chunking

diff --git a/.github/workflows/git_analysis.yml b/.github/workflows/git_analysis.yml
index b40f5be..0b34813 100644
--- a/.github/workflows/git_analysis.yml
+++ b/.github/workflows/git_analysis.yml
@@ -346,7 +346,12 @@ jobs:
         from datetime import datetime
         import google.generativeai as genai
         from google.api_core import exceptions
-        from Docs.config.prompts.meta_template import META_TEMPLATE_PROMPT, VALIDATION_CRITERIA, SECTION_PROMPTS
+        from Docs.config.prompts.meta_template import (
+            META_TEMPLATE_PROMPT,
+            SECTION_PROMPTS,
+            VALIDATION_CRITERIA,
+            assemble_template
+        )
 
         def generate_with_retry(model, prompt, max_retries=3, initial_delay=5):
             for attempt in range(max_retries):
@@ -361,10 +366,19 @@ jobs:
                         raise
             return None
 
+        def refine_section(model, section_name, content):
+            prompt = SECTION_PROMPTS[section_name].format(content=content)
+            return generate_with_retry(model, prompt)
+
         def refine_template(model, template_content):
-            # Use the existing template structure for refinement
-            refinement_content = META_TEMPLATE_PROMPT.format(content=template_content)
-            return generate_with_retry(model, refinement_content)
+            sections = {}
+            # Refine each section separately
+            for section in VALIDATION_CRITERIA.keys():
+                sections[section] = refine_section(model, section, template_content)
+                time.sleep(2)  # Rate limiting
+            
+            # Assemble final template
+            return assemble_template(sections)
 
         # Configure Gemini
         genai.configure(api_key=os.getenv('GOOGLE_API_KEY'))
diff --git a/Docs/config/prompts/meta_template.py b/Docs/config/prompts/meta_template.py
index 59fa094..c603a63 100644
--- a/Docs/config/prompts/meta_template.py
+++ b/Docs/config/prompts/meta_template.py
@@ -1,207 +1,102 @@
-Okay, I understand. I will now generate a comprehensive document adhering to the provided structure and requirements.
+# Base template structure
+BASE_TEMPLATE = """
+# {title}
 
-```
-# Document: Project Chimera: A Computational Trinitarianism Approach to Advanced AI
-
-**Type:** Project Plan & Design Document
+**Type:** {document_type}
 
 **1. Document Header**
-
-**1.1 Title and Type**
-*   **Title:** Project Chimera: A Computational Trinitarianism Approach to Advanced AI
-*   **Type:** Project Plan & Design Document
-
-**1.2 Metadata**
-
-*   **Authors:** AI Document Specialist
-*   **Date:** October 26, 2023
-*   **Version:** 1.0
-*   **Repository:** [Link to Git Repository - Placeholder]
-*   **Hash:** [SHA-256 Hash of Document - Placeholder]
-*   **Category:** Artificial Intelligence, Project Management, Software Development
+{header_content}
 
 **Executive Summary**
+{executive_summary}
+"""
 
-Project Chimera aims to develop an advanced AI system capable of adaptive learning and complex problem-solving through a Computational Trinitarianism framework. This framework separates the system into Logic (abstract specification), Implementation (concrete process), and Evidence (realistic outcomes) layers, fostering a modular and traceable development lifecycle. The project will leverage existing AI models and algorithms, integrate diverse knowledge sources, and prioritize early failure detection to ensure convergence toward a robust and demonstrably valuable AI solution. Success will be measured through specific, predefined metrics related to performance, efficiency, and impact, with a strong emphasis on validation against real-world datasets. The project will be managed using a structured budget and timeline, ensuring clear accountability and efficient resource allocation.
-
-**2. Computational Trinitarianism Framework**
-
-**2.a. Logic Layer (Abstract Specification)**
-
-*   **Context & Vision:**
-    *   **Context:**  The current landscape of AI development is often characterized by monolithic, opaque models. This creates challenges in debugging, updating, and adapting AI systems to new environments.
-    *   **Vision:** Project Chimera envisions a modular, transparent, and adaptable AI system built on the Computational Trinitarianism framework, enabling continuous improvement and streamlined deployment across diverse applications.
-*   **Goals & Functions:**
-    *   **Goal 1:** Develop a robust, adaptive AI capable of solving complex problems in [Specific Domain - e.g., Medical Diagnosis].
-    *   **Function 1.1:**  Data Ingestion and Preprocessing (handle diverse data types, clean data, feature extraction).
-    *   **Function 1.2:**  Knowledge Representation and Reasoning (integrate knowledge graph, inference engine, rule-based system).
-    *   **Function 1.3:**  Learning and Adaptation (implement reinforcement learning, supervised learning, transfer learning).
-    *   **Goal 2:**  Ensure traceability and explainability of AI decisions.
-    *   **Function 2.1:**  Decision Logging (record all decision-making processes).
-    *   **Function 2.2:**  Explainability Module (generate explanations for AI outputs using techniques like LIME or SHAP).
-    *   **Goal 3:**  Enable seamless integration with existing systems.
-    *   **Function 3.1:**  API Design and Implementation (develop a well-defined API for interacting with the AI system).
-    *   **Function 3.2:**  Data Format Standardization (support common data formats like JSON, CSV, and XML).
-
-*   **Success Criteria:**
-    *   **Metric 1:** Accuracy of AI diagnosis in [Specific Domain - e.g., Medical Diagnosis] (Target: 95% accuracy).
-    *   **Metric 2:**  Time to diagnose a patient (Target: Reduction of 50% compared to current methods).
-    *   **Metric 3:**  User satisfaction with explanation clarity (Target: Average score of 4.5 out of 5 on user feedback surveys).
-    *   **Metric 4:**  Integration Time: Time required to integrate the AI system with existing Hospital Information Systems (HIS) (Target: Less than 2 weeks).
-
-*   **Knowledge Integration:**
-    *   Integrate knowledge from:
-        *   Medical databases (e.g., PubMed, MedlinePlus).
-        *   Expert systems in [Specific Domain - e.g., Cardiology].
-        *   Clinical guidelines and best practices.
-        *   Patient data (with appropriate anonymization and security measures).
-
-**2.b. Implementation Layer (Concrete Process)**
-
-*   **Resource Matrix:**
-
-    | Resource          | Description                               | Quantity | Cost (Estimated) | Allocation |
-    | ----------------- | ----------------------------------------- | -------- | ---------------- | ---------- |
-    | Data Scientists   | Expertise in AI/ML                      | 2        | $150,000/year    | 50%        |
-    | Software Engineers| Development and Integration              | 3        | $120,000/year    | 75%        |
-    | Hardware (GPUs)   | High-performance computing for training  | 4        | $5,000/unit      | 100%       |
-    | Cloud Services    | Storage, compute, and API services      | -        | $20,000/year     | 100%       |
-    | Data Acquisition  | Costs associated with acquiring datasets  | -        | $10,000          | 25%        |
-
-*   **Four-Stage Development:**
-    *   **Early Success (Sprint 1-3):**
-        *   **Focus:** Build a basic prototype with core functionality (Data ingestion, basic diagnosis).
-        *   **Deliverables:** Functional prototype, initial dataset integration, preliminary accuracy assessment.
-        *   **Mermaid Diagram:**
-            ```mermaid
-            graph LR
-                A[Data Ingestion] --> B(Feature Extraction);
-                B --> C{Basic Diagnosis Model};
-                C --> D[Output];
-            ```
-        *   **Dependencies:** Access to initial datasets, development environment setup.
-        *   **Assumptions:**  Basic AI models will achieve a minimum accuracy of 70% on initial datasets.
-    *   **Fail Early, Fail Safe (Sprint 4-6):**
-        *   **Focus:** Rigorous testing, identification of failure points, and implementation of mitigation strategies.
-        *   **Deliverables:** Comprehensive test suite, documented error logs, improved model robustness, enhanced data preprocessing.
-        *   **Mermaid Diagram:**
-            ```mermaid
-            graph LR
-                A[Prototype] --> B{Testing};
-                B -- Fail --> C[Failure Analysis & Mitigation];
-                B -- Pass --> D[Continue to Convergence];
-                C --> A;
-            ```
-        *   **Dependencies:** Completed early prototype, access to diverse test datasets.
-        *   **Assumptions:**  Comprehensive testing will uncover key vulnerabilities and limitations in the initial design.
-    *   **Convergence (Sprint 7-9):**
-        *   **Focus:** Refining the AI model, optimizing performance, and integrating knowledge from diverse sources.
-        *   **Deliverables:**  Improved accuracy and efficiency, integrated knowledge graph, enhanced explainability module.
-        *   **Mermaid Diagram:**
-            ```mermaid
-            graph LR
-                A[Refined Model] --> B(Knowledge Integration);
-                B --> C{Performance Optimization};
-                C --> D[Enhanced Explainability];
-                D --> E[Converged AI System];
-            ```
-        *   **Dependencies:** Feedback from testing phase, finalized knowledge graph schema.
-        *   **Assumptions:** Continuous model refinement will lead to significant performance improvements.
-    *   **Demonstration (Sprint 10-12):**
-        *   **Focus:**  Showcasing the AI system's capabilities in a realistic environment and validating its impact.
-        *   **Deliverables:**  Live demonstration with real-world data, user feedback reports, impact assessment.
-        *   **Mermaid Diagram:**
-            ```mermaid
-            graph LR
-                A[Converged AI System] --> B(Real-World Data);
-                B --> C{Live Demonstration};
-                C --> D[User Feedback];
-                D --> E[Impact Assessment];
-            ```
-        *   **Dependencies:** Converged AI system, access to real-world data, user participation.
-        *   **Assumptions:**  The AI system will perform effectively in a real-world setting and demonstrate tangible benefits.
-
-**2.c. Evidence Layer (Realistic Outcomes)**
-
-*   **Measurement Framework:**
-
-    | Metric                 | Description                                              | Target        | Measurement Method                                   | Data Source                      |
-    | ---------------------- | -------------------------------------------------------- | ------------- | ---------------------------------------------------- | -------------------------------- |
-    | Diagnostic Accuracy    | Percentage of correct diagnoses                         | 95%           | Comparison of AI diagnosis with expert diagnosis  | Blinded clinical datasets          |
-    | Time to Diagnose       | Time required for the AI to generate a diagnosis        | 50% reduction | Measurement of processing time                        | System logs                      |
-    | Explanation Clarity   | User satisfaction with explanation of AI decisions      | 4.5/5        | User feedback surveys                                | User questionnaires               |
-    | System Integration Time | Time to connect Chimera to the existing system          | < 2 weeks    | measurement of time it takes to do integration    | System logs                      |
-    | Cost Reduction         | Reduction in diagnostic costs compared to current methods | 20%           | Comparison of costs before and after AI implementation | Hospital financial records         |
-
-*   **Value Realization:**
-    *   Improved diagnostic accuracy leading to better patient outcomes.
-    *   Reduced workload for medical professionals.
-    *   Faster diagnosis and treatment leading to improved patient satisfaction.
-    *   Cost savings through increased efficiency.
-    *   Better access to care for underserved populations.
-    *   Data points, we will collect before and after the AI's implementaion.
-
-*   **Integration Points:**
-    *   Existing Hospital Information System (HIS) via API.
-    *   Electronic Health Records (EHR) system.
-    *   Medical imaging databases.
-    *   Laboratory information systems.
+# Section templates
+HEADER_TEMPLATE = """
+**1.1 Title and Type**
+* **Title:** {title}
+* **Type:** {document_type}
 
-**3. Management Framework**
+**1.2 Metadata**
+* **Authors:** {authors}
+* **Date:** {date}
+* **Version:** {version}
+* **Repository:** {repository}
+* **Hash:** {hash}
+* **Category:** {category}
+"""
 
-*   **Budget Structure:**
-
-    | Category                | Budget (USD) | Control Mechanism                                    |
-    | ----------------------- | ------------ | --------------------------------------------------- |
-    | Personnel Costs         | $510,000     | Timesheet tracking, performance reviews              |
-    | Hardware & Software     | $40,000      | Purchase order approval, inventory management        |
-    | Cloud Services          | $20,000      | Usage monitoring, cost optimization strategies       |
-    | Data Acquisition        | $10,000      | Data licensing agreements, cost-benefit analysis      |
-    | Contingency Fund        | $50,000      | Approval from project sponsor for unforeseen expenses |
-    | **Total**               | **$630,000** |                                                     |
-
-*   **Timeline Management:**
-
-    | Phase                  | Start Date | End Date   | Deliverables                                                                    | Control System                                              |
-    | ---------------------- | ---------- | ---------- | ------------------------------------------------------------------------------ | ----------------------------------------------------------- |
-    | Early Success          | 2023-11-01 | 2024-01-31 | Functional prototype, initial dataset integration                                   | Weekly progress meetings, milestone reviews                  |
-    | Fail Early, Fail Safe | 2024-02-01 | 2024-04-30 | Comprehensive test suite, documented error logs, improved model robustness          | Bi-weekly code reviews, regular testing cycles                 |
-    | Convergence            | 2024-05-01 | 2024-07-31 | Improved accuracy and efficiency, integrated knowledge graph, enhanced explainability | Performance monitoring, knowledge integration audits         |
-    | Demonstration          | 2024-08-01 | 2024-10-31 | Live demonstration with real-world data, user feedback reports, impact assessment | User feedback surveys, stakeholder presentations            |
-    | **Project Completion** |            | 2024-10-31 | Final report, deployed AI system                                                  | Final project review, stakeholder sign-off                   |
-
-*   **Integration Matrix:**
-
-    | Layer        | Integration Point                                                                                    | Technology                                |
-    | ------------- | ------------------------------------------------------------------------------------------------- | ----------------------------------------- |
-    | Logic        | Knowledge Graph integration with reasoning engine                                                 | Neo4j, OWL, SPARQL                      |
-    | Implementation | API integration with HIS and EHR systems                                                            | REST APIs, HL7                             |
-    | Evidence       | Performance monitoring dashboards integrated with system logs                                       | Prometheus, Grafana                     |
-    | Logic/Implementation | Mapping functions from knowledge graph to AI model parameters                                                        | Python, TensorFlow/PyTorch              |
-    | Implementation/Evidence | Validation of model performance against clinical datasets, data will be pushed towards external evidence collection unit. | Python scripts, reporting libraries        |
-    | Logic/Evidence | Explanation of AI decisions based on knowledge graph information.                                         | SHAP, LIME                             |
+FRAMEWORK_TEMPLATE = """
+**2. {framework_name} Framework**
 
-**4. Supporting Documentation**
+**2.a. Logic Layer**
+{logic_content}
 
-*   **References:**
-    *   [Link to Research Paper on Explainable AI]
-    *   [Link to Medical Database Documentation]
-    *   [Link to relevant industry standards (e.g., HL7 for healthcare data)]
+**2.b. Implementation Layer**
+{implementation_content}
 
-*   **Change History:**
+**2.c. Evidence Layer**
+{evidence_content}
+"""
 
-    | Version | Date       | Author             | Description                                                                                                             |
-    | ------- | ---------- | ------------------ | --------------------------------------------------------------------------------------------------------------------- |
-    | 0.1     | 2023-10-25 | AI Document Specialist | Initial Draft                                                                                                               |
-    | 1.0     | 2023-10-26 | AI Document Specialist | Added Executive Summary, Mermaid Diagrams, Measurable Metrics, Integration Matrix, and completed all sections. |
-```
+MANAGEMENT_TEMPLATE = """
+**3. Management Framework**
+* **Budget Structure:**
+{budget_content}
 
-**Explanation of Design Choices and Compliance:**
+* **Timeline Management:**
+{timeline_content}
 
-*   **Completeness:** All sections are populated with relevant content.
-*   **Consistency:** The framework maintains alignment, demonstrating how abstract goals translate to concrete implementation and measurable outcomes. For example, the goal of accurate diagnosis is linked to the function of data ingestion and learning, implemented through specific algorithms and measured by a diagnostic accuracy metric.
-*   **Measurability:** Clear, quantifiable metrics are defined (e.g., 95% diagnostic accuracy, 50% reduction in diagnosis time).
-*   **Traceability:** The document establishes clear links between requirements, implementation steps, and evidence points.  For example, the requirement for explainability is linked to the implementation of an explainability module and the measurement of user satisfaction with explanation clarity.
-*   **Integration:** The integration matrix clearly demonstrates connections between the Logic, Implementation, and Evidence layers, showing how data and insights flow between them.  Dependencies and assumptions are clearly stated within each developmental stage.
+* **Integration Matrix:**
+{integration_content}
+"""
 
-This comprehensive document provides a robust foundation for Project Chimera, guiding the development of an advanced AI system using the Computational Trinitarianism framework.  It includes all the requested elements: mermaid diagrams, measurable metrics, traceability, documented dependencies, and clear section integration. The placeholder links need to be replaced with actual links for a production-ready document. Remember to replace the bracketed information with accurate, specific details for your project.
+DOCUMENTATION_TEMPLATE = """
+**4. Supporting Documentation**
+* **References:**
+{references}
+
+* **Change History:**
+{change_history}
+"""
+
+# Validation criteria for each section
+VALIDATION_CRITERIA = {
+    'header': ['title', 'type', 'metadata'],
+    'executive_summary': ['context', 'goals', 'approach', 'expected_outcomes'],
+    'framework': ['logic', 'implementation', 'evidence'],
+    'management': ['budget', 'timeline', 'integration'],
+    'documentation': ['references', 'changes']
+}
+
+# Section-specific prompts
+SECTION_PROMPTS = {
+    'header': 'Generate a document header with title, type, and metadata...',
+    'executive_summary': 'Create a concise executive summary that covers...',
+    'framework': 'Develop a framework section that includes...',
+    'management': 'Structure the management section with...',
+    'documentation': 'Compile supporting documentation including...'
+}
+
+# Template assembly function
+def assemble_template(sections):
+    return '\n'.join([
+        BASE_TEMPLATE,
+        FRAMEWORK_TEMPLATE,
+        MANAGEMENT_TEMPLATE,
+        DOCUMENTATION_TEMPLATE
+    ]).format(**sections)
+
+# Meta template prompt
+META_TEMPLATE_PROMPT = """
+Please analyze the following content and generate a refined version following these guidelines:
+
+1. Structure: Follow the template structure defined above
+2. Sections: Ensure all required sections are present and properly formatted
+3. Integration: Maintain clear connections between different layers
+4. Metrics: Include specific, measurable outcomes
+5. Visualization: Add relevant diagrams where appropriate
+
+Content to analyze:
+{content}
+"""
\ No newline at end of file

commit 48ae0a3fee0e2c94755eae89bdb82d1e518ddc70
Author: daffa <daffa.padantya12@gmail.com>
Date:   Wed Mar 5 14:46:41 2025 +0800

    refinedment add

diff --git a/.github/workflows/git_analysis.yml b/.github/workflows/git_analysis.yml
index 59cdfdb..f2e6c60 100644
--- a/.github/workflows/git_analysis.yml
+++ b/.github/workflows/git_analysis.yml
@@ -91,7 +91,6 @@ jobs:
         import time
         from datetime import datetime
         import google.generativeai as genai
-        from google.api_core import exceptions
         from Docs.config.prompts.group_analysis import GROUP_ANALYSIS_PROMPT
         from Docs.config.prompts.user_analysis import USER_ANALYSIS_PROMPT
         from Docs.config.prompts.summary import SUMMARY_PROMPT
@@ -206,13 +205,82 @@ jobs:
         cat << 'EOF' > refine_analysis.py
         import os
         import glob
+        import time
         from datetime import datetime
         import google.generativeai as genai
+        from google.api_core import exceptions
         from Docs.config.prompts.group_critique import GROUP_CRITIQUE_PROMPT
         from Docs.config.prompts.user_critique import USER_CRITIQUE_PROMPT
         from Docs.config.prompts.refinement import REFINEMENT_PROMPT
 
-        # Replace hardcoded prompts with imported ones
+        def generate_with_retry(model, prompt, max_retries=3, initial_delay=5):
+            for attempt in range(max_retries):
+                try:
+                    if attempt > 0:
+                        time.sleep(initial_delay * (2 ** attempt))
+                    response = model.generate_content(prompt)
+                    return response.text
+                except exceptions.ResourceExhausted:
+                    if attempt == max_retries - 1:
+                        raise
+                    print(f"Rate limit hit, retrying in {initial_delay * (2 ** (attempt + 1))} seconds...")
+                except Exception as e:
+                    print(f"Error: {str(e)}")
+                    if attempt == max_retries - 1:
+                        raise
+            return None
+
+        def refine_analysis(model, analysis_content, critique_prompt):
+            # Generate critique
+            critique = generate_with_retry(model, critique_prompt)
+            if not critique:
+                return analysis_content
+
+            # Use critique to refine
+            refined = generate_with_retry(
+                model,
+                REFINEMENT_PROMPT.format(
+                    analysis_content=analysis_content,
+                    critique=critique
+                )
+            )
+            return refined if refined else analysis_content
+
+        # Configure Gemini
+        genai.configure(api_key=os.getenv('GOOGLE_API_KEY'))
+        model = genai.GenerativeModel('gemini-2.0-flash')
+
+        # Refine group analysis
+        group_files = glob.glob('Docs/analysis/group/team-analysis-*.md')
+        if group_files:
+            latest_analysis = max(group_files)
+            with open(latest_analysis, 'r') as f:
+                analysis_content = f.read()
+            
+            refined_analysis = refine_analysis(model, analysis_content, GROUP_CRITIQUE_PROMPT)
+            if refined_analysis:
+                refined_path = latest_analysis.replace('team-analysis-', 'refined-team-analysis-')
+                with open(refined_path, 'w') as f:
+                    f.write(f"# Refined Team Analysis\nGenerated at: {datetime.now()}\n\n{refined_analysis}")
+
+        # Refine individual analyses
+        user_dirs = glob.glob('Docs/analysis/users/*/')
+        for user_dir in user_dirs:
+            username = os.path.basename(os.path.dirname(user_dir))
+            if username == '.gitkeep':
+                continue
+
+            analysis_files = glob.glob(f'{user_dir}analysis-*.md')
+            if analysis_files:
+                latest_analysis = max(analysis_files)
+                with open(latest_analysis, 'r') as f:
+                    analysis_content = f.read()
+
+                refined_analysis = refine_analysis(model, analysis_content, USER_CRITIQUE_PROMPT)
+                if refined_analysis:
+                    refined_path = latest_analysis.replace('analysis-', 'refined-analysis-')
+                    with open(refined_path, 'w') as f:
+                        f.write(f"# Refined Developer Analysis - {username}\nGenerated at: {datetime.now()}\n\n{refined_analysis}")
         EOF
 
         python refine_analysis.py

commit de3aa9ee30c5938c2bc5048445e35185ca096b65
Author: daffa <daffa.padantya12@gmail.com>
Date:   Wed Mar 5 14:34:33 2025 +0800

    quota exceeded fix

diff --git a/.github/workflows/git_analysis.yml b/.github/workflows/git_analysis.yml
index 176a97b..59cdfdb 100644
--- a/.github/workflows/git_analysis.yml
+++ b/.github/workflows/git_analysis.yml
@@ -91,10 +91,53 @@ jobs:
         import time
         from datetime import datetime
         import google.generativeai as genai
+        from google.api_core import exceptions
         from Docs.config.prompts.group_analysis import GROUP_ANALYSIS_PROMPT
         from Docs.config.prompts.user_analysis import USER_ANALYSIS_PROMPT
         from Docs.config.prompts.summary import SUMMARY_PROMPT
 
+        def generate_with_retry(model, prompt, max_retries=3, initial_delay=5):
+            for attempt in range(max_retries):
+                try:
+                    if attempt > 0:
+                        time.sleep(initial_delay * (2 ** attempt))  # Exponential backoff
+                    response = model.generate_content(prompt)
+                    return response.text
+                except exceptions.ResourceExhausted:
+                    if attempt == max_retries - 1:
+                        raise
+                    print(f"Rate limit hit, retrying in {initial_delay * (2 ** (attempt + 1))} seconds...")
+                except Exception as e:
+                    print(f"Error: {str(e)}")
+                    if attempt == max_retries - 1:
+                        raise
+            return None
+
+        def analyze_content(model, content, query, prompt_template):
+            chunks = chunk_content(content)
+            all_analyses = []
+            
+            for i, chunk in enumerate(chunks, 1):
+                if i > 1:
+                    time.sleep(5)  # Increased delay between requests
+                
+                chunk_prompt = prompt_template.format(
+                    query=query,
+                    content=chunk,
+                    chunk_info=f"(Part {i} of {len(chunks)})" if len(chunks) > 1 else ""
+                )
+                
+                analysis = generate_with_retry(model, chunk_prompt)
+                if analysis:
+                    all_analyses.append(analysis)
+            
+            if len(all_analyses) > 1:
+                time.sleep(5)  # Increased delay before summary
+                summary_prompt = SUMMARY_PROMPT.format(content='\n\n'.join(all_analyses))
+                return generate_with_retry(model, summary_prompt)
+            
+            return all_analyses[0] if all_analyses else "Analysis failed due to API limitations"
+
         def chunk_content(content, max_chars=400000):  # Approximately 100k tokens
             lines = content.split('\n')
             chunks = []
@@ -115,31 +158,6 @@ jobs:
                 chunks.append('\n'.join(current_chunk))
             return chunks
 
-        def analyze_content(model, content, query, prompt_template):
-            chunks = chunk_content(content)
-            all_analyses = []
-            
-            for i, chunk in enumerate(chunks, 1):
-                if i > 1:
-                    time.sleep(2)
-                
-                chunk_prompt = prompt_template.format(
-                    query=query,
-                    content=chunk,
-                    chunk_info=f"(Part {i} of {len(chunks)})" if len(chunks) > 1 else ""
-                )
-                
-                response = model.generate_content(chunk_prompt)
-                all_analyses.append(response.text)
-            
-            if len(all_analyses) > 1:
-                time.sleep(2)
-                summary_prompt = SUMMARY_PROMPT.format(content='\n\n'.join(all_analyses))
-                final_response = model.generate_content(summary_prompt)
-                return final_response.text
-            
-            return all_analyses[0]
-
         # Configure Gemini
         genai.configure(api_key="AIzaSyBZ52gRnYBjfyyh4jiEWscKoRfTx-j4YEQ")
         model = genai.GenerativeModel('gemini-2.0-flash')

commit f884a27ec955510d30a58d3d34e613394d022cca
Author: daffa <daffa.padantya12@gmail.com>
Date:   Wed Mar 5 14:29:10 2025 +0800

    prompt modularity

diff --git a/.github/workflows/git_analysis.yml b/.github/workflows/git_analysis.yml
index 9f47e66..176a97b 100644
--- a/.github/workflows/git_analysis.yml
+++ b/.github/workflows/git_analysis.yml
@@ -91,6 +91,9 @@ jobs:
         import time
         from datetime import datetime
         import google.generativeai as genai
+        from Docs.config.prompts.group_analysis import GROUP_ANALYSIS_PROMPT
+        from Docs.config.prompts.user_analysis import USER_ANALYSIS_PROMPT
+        from Docs.config.prompts.summary import SUMMARY_PROMPT
 
         def chunk_content(content, max_chars=400000):  # Approximately 100k tokens
             lines = content.split('\n')
@@ -117,9 +120,8 @@ jobs:
             all_analyses = []
             
             for i, chunk in enumerate(chunks, 1):
-                # Add delay between API calls
                 if i > 1:
-                    time.sleep(2)  # 2 second delay between requests
+                    time.sleep(2)
                 
                 chunk_prompt = prompt_template.format(
                     query=query,
@@ -131,15 +133,8 @@ jobs:
                 all_analyses.append(response.text)
             
             if len(all_analyses) > 1:
-                # Add delay before summary request
                 time.sleep(2)
-                summary_prompt = f"""
-                Synthesize these separate analyses into one coherent analysis:
-
-                {'\n\n'.join(all_analyses)}
-
-                Provide a unified analysis that covers all parts.
-                """
+                summary_prompt = SUMMARY_PROMPT.format(content='\n\n'.join(all_analyses))
                 final_response = model.generate_content(summary_prompt)
                 return final_response.text
             
@@ -157,19 +152,7 @@ jobs:
                 group_content = f.read()
 
             query = '${{ github.event.inputs.query }}'
-            group_prompt_template = """
-            Analyze this team's git log {chunk_info} and {query}:
-
-            {content}
-
-            Please provide:
-            1. A summary of key changes
-            2. Team collaboration patterns
-            3. Project progress analysis
-            4. Recommendations for the team
-            """
-
-            analysis = analyze_content(model, group_content, query, group_prompt_template)
+            analysis = analyze_content(model, group_content, query, GROUP_ANALYSIS_PROMPT)
             os.makedirs('Docs/analysis/group', exist_ok=True)
             with open(f'Docs/analysis/group/team-analysis-{datetime.now().strftime("%Y-%m-%d")}.md', 'w') as f:
                 f.write(f"# Team Analysis\nGenerated at: {datetime.now()}\n\n{analysis}")
@@ -187,19 +170,10 @@ jobs:
                 with open(latest_user_log, 'r') as f:
                     user_content = f.read()
 
-                user_prompt = f"""
-                Analyze this developer's git activity and {query}:
-
-                {user_content}
-
-                Please provide:
-                1. Individual contribution summary
-                2. Work patterns and focus areas
-                3. Technical expertise demonstrated
-                4. Specific recommendations
-                """
-
-                response = model.generate_content(user_prompt)
+                response = model.generate_content(USER_ANALYSIS_PROMPT.format(
+                    query=query,
+                    content=user_content
+                ))
                 os.makedirs(f'Docs/analysis/users/{username}', exist_ok=True)
                 with open(f'Docs/analysis/users/{username}/analysis-{datetime.now().strftime("%Y-%m-%d")}.md', 'w') as f:
                     f.write(f"# Developer Analysis - {username}\nGenerated at: {datetime.now()}\n\n{response.text}")
@@ -216,81 +190,11 @@ jobs:
         import glob
         from datetime import datetime
         import google.generativeai as genai
+        from Docs.config.prompts.group_critique import GROUP_CRITIQUE_PROMPT
+        from Docs.config.prompts.user_critique import USER_CRITIQUE_PROMPT
+        from Docs.config.prompts.refinement import REFINEMENT_PROMPT
 
-        # Configure Gemini
-        genai.configure(api_key="AIzaSyBZ52gRnYBjfyyh4jiEWscKoRfTx-j4YEQ")
-        model = genai.GenerativeModel('gemini-2.0-flash')
-
-        def refine_with_critique(analysis_content, critique_prompt):
-            # First get critique
-            critique_response = model.generate_content(critique_prompt)
-            critique = critique_response.text
-
-            # Use critique to refine original analysis
-            refine_prompt = f"""
-            Here is the original analysis:
-            {analysis_content}
-
-            Here is the critique of this analysis:
-            {critique}
-
-            Please provide a refined and improved analysis that:
-            1. Addresses all the critical feedback points
-            2. Incorporates the additional insights
-            3. Enhances the recommendations
-            4. Fixes any identified gaps or inaccuracies
-
-            Format the response as a complete, standalone analysis report.
-            """
-
-            refined_response = model.generate_content(refine_prompt)
-            return refined_response.text
-
-        # Refine group analysis
-        group_files = glob.glob('Docs/analysis/group/*.md')
-        if group_files:
-            latest_analysis = max(group_files)
-            with open(latest_analysis, 'r') as f:
-                analysis_content = f.read()
-
-            critique_prompt = f"""
-            Review and critique this analysis, focusing on:
-            1. Accuracy of observations
-            2. Depth of insights
-            3. Actionability of recommendations
-            4. Missing important patterns
-            
-            Provide specific, detailed feedback on each aspect.
-            """
-
-            refined_analysis = refine_with_critique(analysis_content, critique_prompt)
-            with open(f'Docs/analysis/group/refined-team-analysis-{datetime.now().strftime("%Y-%m-%d")}.md', 'w') as f:
-                f.write(f"# Refined Team Analysis\nGenerated at: {datetime.now()}\n\n{refined_analysis}")
-
-        # Refine individual analyses
-        user_dirs = glob.glob('Docs/analysis/users/*/')
-        for user_dir in user_dirs:
-            username = os.path.basename(os.path.dirname(user_dir))
-            analysis_files = glob.glob(f'{user_dir}analysis-*.md')
-            
-            if analysis_files:
-                latest_analysis = max(analysis_files)
-                with open(latest_analysis, 'r') as f:
-                    analysis_content = f.read()
-
-                critique_prompt = f"""
-                Review and critique this developer analysis, focusing on:
-                1. Accuracy of contribution assessment
-                2. Depth of technical insights
-                3. Relevance of recommendations
-                4. Missing patterns in work style
-                
-                Provide specific, detailed feedback on each aspect.
-                """
-
-                refined_analysis = refine_with_critique(analysis_content, critique_prompt)
-                with open(f'{user_dir}refined-analysis-{datetime.now().strftime("%Y-%m-%d")}.md', 'w') as f:
-                    f.write(f"# Refined Developer Analysis - {username}\nGenerated at: {datetime.now()}\n\n{refined_analysis}")
+        # Replace hardcoded prompts with imported ones
         EOF
 
         python refine_analysis.py
diff --git a/Docs/config/prompts/group_analysis.py b/Docs/config/prompts/group_analysis.py
new file mode 100644
index 0000000..15f7fe2
--- /dev/null
+++ b/Docs/config/prompts/group_analysis.py
@@ -0,0 +1,11 @@
+GROUP_ANALYSIS_PROMPT = """
+Analyze this team's git log {chunk_info} and {query}:
+
+{content}
+
+Please provide:
+1. A summary of key changes
+2. Team collaboration patterns
+3. Project progress analysis
+4. Recommendations for the team
+"""
\ No newline at end of file
diff --git a/Docs/config/prompts/group_critique.py b/Docs/config/prompts/group_critique.py
new file mode 100644
index 0000000..76a11cd
--- /dev/null
+++ b/Docs/config/prompts/group_critique.py
@@ -0,0 +1,9 @@
+GROUP_CRITIQUE_PROMPT = """
+Review and critique this analysis, focusing on:
+1. Accuracy of observations
+2. Depth of insights
+3. Actionability of recommendations
+4. Missing important patterns
+
+Provide specific, detailed feedback on each aspect.
+"""
\ No newline at end of file
diff --git a/Docs/config/prompts/refinement.py b/Docs/config/prompts/refinement.py
new file mode 100644
index 0000000..eea52d9
--- /dev/null
+++ b/Docs/config/prompts/refinement.py
@@ -0,0 +1,15 @@
+REFINEMENT_PROMPT = """
+Here is the original analysis:
+{analysis_content}
+
+Here is the critique of this analysis:
+{critique}
+
+Please provide a refined and improved analysis that:
+1. Addresses all the critical feedback points
+2. Incorporates the additional insights
+3. Enhances the recommendations
+4. Fixes any identified gaps or inaccuracies
+
+Format the response as a complete, standalone analysis report.
+"""
\ No newline at end of file
diff --git a/Docs/config/prompts/summary.py b/Docs/config/prompts/summary.py
new file mode 100644
index 0000000..8fd0b99
--- /dev/null
+++ b/Docs/config/prompts/summary.py
@@ -0,0 +1,7 @@
+SUMMARY_PROMPT = """
+Synthesize these separate analyses into one coherent analysis:
+
+{content}
+
+Provide a unified analysis that covers all parts.
+"""
\ No newline at end of file
diff --git a/Docs/config/prompts/user_analysis.py b/Docs/config/prompts/user_analysis.py
new file mode 100644
index 0000000..6ca6f50
--- /dev/null
+++ b/Docs/config/prompts/user_analysis.py
@@ -0,0 +1,11 @@
+USER_ANALYSIS_PROMPT = """
+Analyze this developer's git activity and {query}:
+
+{content}
+
+Please provide:
+1. Individual contribution summary
+2. Work patterns and focus areas
+3. Technical expertise demonstrated
+4. Specific recommendations
+"""
\ No newline at end of file
diff --git a/Docs/config/prompts/user_critique.py b/Docs/config/prompts/user_critique.py
new file mode 100644
index 0000000..6bb554d
--- /dev/null
+++ b/Docs/config/prompts/user_critique.py
@@ -0,0 +1,9 @@
+USER_CRITIQUE_PROMPT = """
+Review and critique this developer analysis, focusing on:
+1. Accuracy of contribution assessment
+2. Depth of technical insights
+3. Relevance of recommendations
+4. Missing patterns in work style
+
+Provide specific, detailed feedback on each aspect.
+"""
\ No newline at end of file
```
